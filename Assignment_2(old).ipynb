{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8146e0",
   "metadata": {},
   "source": [
    "# Programming and Tools for AI (/Online)\n",
    "# James McDermott, University of Galway, 2022\n",
    "# CT5132/CT5148\n",
    "# Assignment 2\n",
    "\n",
    "**Due**:\n",
    "\n",
    "* CT5132 midnight Friday 28 Oct (end of Week 8)\n",
    "* CT5148 midnight Sunday 30 Oct (end of Week 8)\n",
    "\n",
    "**Weight**: this is worth 25% of the module.\n",
    "\n",
    "**Groups**: you can work solo or in a group of two, as you prefer. You can not work together with another student you already worked with in any assignment in this module or any other. If working in a group, both students should submit and their submissions should be identical.\n",
    "\n",
    "**Plagiarism**: students are reminded of the University's policies on plagiarism. Code copied from the internet should be sourced with the URL in a comment. Students can discuss the assignment but must not look at each other's work (other than within a group). Some cases were reported to relevant authorities during Assignment 1.\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "* Add your name(s)/ID(s) in the next cell.\n",
    "* Write code in this notebook to pass the doctests and produce the images as directed.\n",
    "* Write new doctests. For each docstring below which contains doctests, you have to add one more doctest inside the same docstring, together with a line or two of explanation in your own words.\n",
    "* Submit your .ipynb file and nothing else. Don't type in the submission box.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "Do not change or delete any doctests, or insert lines above the docstrings, or change the doctest calls.\n",
    "\n",
    "When I receive your submission, I'll go to the Kernel menu and \"Restart and run all\", and I'll look at your code, your doctests, and your outputs. So, the last thing you should do before submitting is \"Restart and run all\" and check that every part runs correctly. (One thing that sometimes goes wrong in notebooks is that a piece of code relies on a variable or function which was previously defined but is now deleted but is still \"alive\" in the kernel's memory, so the notebook crashes when re-run.)\n",
    "\n",
    "Marks will be awarded for each of the 12 questions. Both correct outputs and code will be assessed. Images are not expected to match exactly, but doctests are.\n",
    "\n",
    "If you get stuck on any part, you should still be able to continue to other parts. If you can't see how to proceed in this case, please ask me.\n",
    "\n",
    "There are a lot of parts, so even if you don't get everything, you can still get a good mark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65f5c6",
   "metadata": {},
   "source": [
    "**Student ID(s)**: 22222806, 22229629\n",
    "\n",
    "**Name(s)**: Jash Prakash Rana, Manas Ballal Atre\n",
    "\n",
    "**Declaration**: By submitting to Blackboard, I/we declare that I/we have not seen any work by other students on this assignment and have not shown my/our work to any others. I/we declare that we have not worked together on any previous assignment in this module or another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed25ca",
   "metadata": {},
   "source": [
    "# Clustering Sequences using $n$-grams\n",
    "\n",
    "In this assignment, we'll put together a clustering method specialised to sequence mining, in the style of Scikit-Learn. We'll use components from Scikit-Learn itself, Numpy, and Matplotlib.\n",
    "\n",
    "Usually when we do machine learning, we fit with numerical, rectangular data, so each sample is a feature vector. But sequences are a bit different. For a start, each sequence might be of different length. Also, commonalities between sequences might be mis-aligned, eg these two sequences are pretty similar:\n",
    "\n",
    "$(0, 0, 1, 4, 7, 0, 0, 1)$\n",
    "\n",
    "$(0, 1, 4, 7, 0, 1)$\n",
    "\n",
    "The approach we'll use is to count common subsequences of length $n$, ie $n$-grams. \n",
    "\n",
    "Many machine learning algorithms work fine even if we don't have feature vectors, but we have distances between points. \n",
    "\n",
    "We will use the $n$-gram counts to compute a measure of dissimilarity between any pair of sequences.\n",
    "\n",
    "In Scikit-Learn, several algorithms accept a keyword such as `metric=\"precomputed\"` or `affinity=\"precomputed\"`, which allows us to pass in the square matrix of distances instead of passing in the points themselves.\n",
    "\n",
    "This notebook will walk us through all the steps, with doctests and examples for each. Most of the code needed is based on something we've seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db49713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports - we don't need any others\n",
    "import sys\n",
    "assert sys.version_info >= (3, 7), \"This notebook requires Python 3.7+\"\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import doctest\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a022f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 0, 1, 0, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 2, 2, 2]\n",
      "[4, 4, 3, 2, 3, 1, 2, 4, 5, 3, 6, 5, 5, 5, 6, 6, 4, 5, 2, 6, 1, 6, 6, 3, 3, 2, 1, 6, 1, 1, 2, 5, 5, 2, 3, 6, 5, 6, 3, 3]\n",
      "[2, 2, 2, 4, 4, 2, 2, 3, 4, 4, 3, 4, 3, 2, 4, 2, 2, 4, 3, 2, 4, 2, 4, 3, 3, 3, 2, 2, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "# here we construct 10 fake sequences from each of 3 methods\n",
    "X = [[random.randrange(0, 3) for _ in range(40)] for _ in range(50)] + \\\n",
    "    [[random.randrange(1, 7) for _ in range(40)] for _ in range(50)] + \\\n",
    "    [[random.randrange(2, 5) for _ in range(30)] for _ in range(50)]\n",
    "print(X[0]) # take a look at one example from each method\n",
    "print(X[50])\n",
    "print(X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aaae20",
   "metadata": {},
   "source": [
    "**Question 1**. Suppose we have a sequence of integers, `x`, eg `x = [0, 1, 4, 0, 1, 4, 7, 0, 1, 4]`. We immediately see that there are some common subsequences: eg `[0, 1, 4]` occurs three times.\n",
    "\n",
    "Write a function `count_ngrams` which extracts all *n-grams*, ie all subsequences of length `n`, and returns their number of occurrences in a `Counter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b078cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(x, n):\n",
    "    '''\n",
    "The input can be a list:\n",
    ">>> count_ngrams([0, 1, 4, 0, 1, 4, 7, 0, 1, 4], 3)\n",
    "Counter({(0, 1, 4): 3,\n",
    "         (1, 4, 0): 1,\n",
    "         (4, 0, 1): 1,\n",
    "         (1, 4, 7): 1,\n",
    "         (4, 7, 0): 1,\n",
    "         (7, 0, 1): 1})\n",
    "         \n",
    "It can be a tuple:\n",
    ">>> count_ngrams((0, 1, 4, 0, 1, 4, 7, 0, 1, 4), 5)\n",
    "Counter({(0, 1, 4, 0, 1): 1, \n",
    "         (1, 4, 0, 1, 4): 1, \n",
    "         (4, 0, 1, 4, 7): 1, \n",
    "         (0, 1, 4, 7, 0): 1, \n",
    "         (1, 4, 7, 0, 1): 1, \n",
    "         (4, 7, 0, 1, 4): 1})\n",
    "         \n",
    "It can be empty:         \n",
    ">>> count_ngrams([], 3)\n",
    "Counter()\n",
    "\n",
    "It can even be a string:\n",
    ">>> count_ngrams(\"abcdabcdefacd\", 3)\n",
    "Counter({('a', 'b', 'c'): 2, ('b', 'c', 'd'): 2, ('c', 'd', 'a'): 1, \n",
    "         ('d', 'a', 'b'): 1, ('c', 'd', 'e'): 1, ('d', 'e', 'f'): 1, \n",
    "         ('e', 'f', 'a'): 1, ('f', 'a', 'c'): 1, ('a', 'c', 'd'): 1})\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    input = x\n",
    "    ngram_val = n\n",
    "\n",
    "    #initiailzing temp variable as a list, to use the append function\n",
    "    temp = []\n",
    "\n",
    "    for i in range((len(input) - ngram_val) + 1):\n",
    "        #appending each item of 'input' into the temp variable\n",
    "        temp.append(input[i:i + ngram_val])\n",
    "        #converting each ngram created to tuple for hashing into the Counter\n",
    "        temp[i] = tuple(temp[i])\n",
    "    result = Counter(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fcd135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    count_ngrams([0, 1, 4, 0, 1, 4, 7, 0, 1, 4], 3)\n",
      "Expecting:\n",
      "    Counter({(0, 1, 4): 3,\n",
      "             (1, 4, 0): 1,\n",
      "             (4, 0, 1): 1,\n",
      "             (1, 4, 7): 1,\n",
      "             (4, 7, 0): 1,\n",
      "             (7, 0, 1): 1})\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams((0, 1, 4, 0, 1, 4, 7, 0, 1, 4), 5)\n",
      "Expecting:\n",
      "    Counter({(0, 1, 4, 0, 1): 1, \n",
      "             (1, 4, 0, 1, 4): 1, \n",
      "             (4, 0, 1, 4, 7): 1, \n",
      "             (0, 1, 4, 7, 0): 1, \n",
      "             (1, 4, 7, 0, 1): 1, \n",
      "             (4, 7, 0, 1, 4): 1})\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams([], 3)\n",
      "Expecting:\n",
      "    Counter()\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams(\"abcdabcdefacd\", 3)\n",
      "Expecting:\n",
      "    Counter({('a', 'b', 'c'): 2, ('b', 'c', 'd'): 2, ('c', 'd', 'a'): 1, \n",
      "             ('d', 'a', 'b'): 1, ('c', 'd', 'e'): 1, ('d', 'e', 'f'): 1, \n",
      "             ('e', 'f', 'a'): 1, ('f', 'a', 'c'): 1, ('a', 'c', 'd'): 1})\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(count_ngrams, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a5caa",
   "metadata": {},
   "source": [
    "**Question 2**. Suppose we have a list of sequences, `X`. Write a function which gets the ngrams for each sequence in `X`. Of course it should just use our `count_ngrams`. Notice that `X` could be *ragged*, ie not every sequence in `X` has to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13df3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams_multi(X, n):\n",
    "    '''\n",
    ">>> count_ngrams_multi([[0, 1, 4, 0, 1, 4, 7, 0, 1, 4],\n",
    "...                     [4, 5, 6, 4, 5, 6],\n",
    "...                     [1, 5, 9, 5, 1]], 3)\n",
    "[Counter({(0, 1, 4): 3, (1, 4, 0): 1, (4, 0, 1): 1, \n",
    "          (1, 4, 7): 1, (4, 7, 0): 1, (7, 0, 1): 1}), \n",
    " Counter({(4, 5, 6): 2, (5, 6, 4): 1, (6, 4, 5): 1}), \n",
    " Counter({(1, 5, 9): 1, (5, 9, 5): 1, (9, 5, 1): 1})]\n",
    ">>> count_ngrams_multi([[]], 3)\n",
    "[Counter()]\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    input = X\n",
    "    ngram_val = n\n",
    "\n",
    "    #initiailzing temp variable as a list, to use the append function\n",
    "    temp = []\n",
    "    result = []\n",
    "    counter = Counter()\n",
    "\n",
    "    #First for loop for iterating the 2D list\n",
    "    for var in input:\n",
    "        for i in range((len(var) - ngram_val) + 1):\n",
    "            #appending each item of 'input' into the temp variable\n",
    "            temp.append(var[i:i + ngram_val])\n",
    "            #converting each ngram created to tuple for hashing into the Counter\n",
    "            temp[i] = tuple(temp[i])\n",
    "            counter = Counter(temp)\n",
    "        temp = []\n",
    "\n",
    "        #Storing each multi-list counter in another list\n",
    "        result.append(counter)\n",
    "\n",
    "        #Clearing the counter to refresh for another list\n",
    "        counter += Counter()   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d34f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    count_ngrams_multi([[0, 1, 4, 0, 1, 4, 7, 0, 1, 4],\n",
      "                        [4, 5, 6, 4, 5, 6],\n",
      "                        [1, 5, 9, 5, 1]], 3)\n",
      "Expecting:\n",
      "    [Counter({(0, 1, 4): 3, (1, 4, 0): 1, (4, 0, 1): 1, \n",
      "              (1, 4, 7): 1, (4, 7, 0): 1, (7, 0, 1): 1}), \n",
      "     Counter({(4, 5, 6): 2, (5, 6, 4): 1, (6, 4, 5): 1}), \n",
      "     Counter({(1, 5, 9): 1, (5, 9, 5): 1, (9, 5, 1): 1})]\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams_multi([[]], 3)\n",
      "Expecting:\n",
      "    [Counter()]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# This runs our count_ngrams_multi doctests.\n",
    "doctest.run_docstring_examples(count_ngrams_multi, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76a14b",
   "metadata": {},
   "source": [
    "**Question 3**. We're going to need a helper function `counter_total` which measures the \"total size\" of a `Counter` by counting the total of its individual counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9f6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_total(c):\n",
    "    '''\n",
    "    Sum of the counts in a counter c\n",
    "    \n",
    "    If you have Python 3.10+, this is available directly as c.total().\n",
    "    \n",
    "    >>> counter_total(Counter())\n",
    "    0\n",
    "    >>> counter_total(Counter({'a': 1, 'b': 3}))\n",
    "    4\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    return sum(val for val in c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33442be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    counter_total(Counter())\n",
      "Expecting:\n",
      "    0\n",
      "ok\n",
      "Trying:\n",
      "    counter_total(Counter({'a': 1, 'b': 3}))\n",
      "Expecting:\n",
      "    4\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(counter_total, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd121ec",
   "metadata": {},
   "source": [
    "**Question 4**.\n",
    "\n",
    "Next, we want to measure the *dissimilarity* between two sequences, `X` and `Y`.  \n",
    "\n",
    "Since we already have a method for representing a sequence as a `Counter` of n-grams, we only need a method for measuring the dissimilarity of two `Counter`s, $A$ and $B$. One main approach is the *Jaccard dissimilarity*. It's defined as:\n",
    "\n",
    "$$\\mathrm{JD}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Jaccard_index.)\n",
    "\n",
    "In other words, $\\mathrm{JD}$ is low (dissimilarity low => similarity high) if there are many $n$-grams in common between `X` and `Y`. \n",
    "\n",
    "By the way, if you Google for \"python jaccard distance\" you'll find a lot of results which are not what we want, so be careful.\n",
    "\n",
    "To calculate intersection and union on `Counter` objects we can use `&` and `|` operators, and the result is a new `Counter`. After that we can use our `counter_total` to get the size `| |`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d95feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JD(A, B):\n",
    "    '''Jaccard dissimilarity on multisets, represented by Counters.\n",
    "    \n",
    "    Two are identical:\n",
    "    >>> JD(Counter({0, 1, 2, 3}), Counter({0, 1, 2, 3}))\n",
    "    0.0\n",
    "    \n",
    "    Two are similar:\n",
    "    >>> JD(Counter([0, 0, 0, 1, 1, 2, 3]), Counter([0, 0, 0, 1, 1, 2, 4]))\n",
    "    0.25\n",
    "    \n",
    "    Two are totally different:\n",
    "    >>> JD(Counter({0, 1, 2, 3}), Counter({10, 11, 12, 13}))\n",
    "    1.0\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    return 1 - (abs(counter_total(A & B)) / abs(counter_total(A | B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf78c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    JD(Counter({0, 1, 2, 3}), Counter({0, 1, 2, 3}))\n",
      "Expecting:\n",
      "    0.0\n",
      "ok\n",
      "Trying:\n",
      "    JD(Counter([0, 0, 0, 1, 1, 2, 3]), Counter([0, 0, 0, 1, 1, 2, 4]))\n",
      "Expecting:\n",
      "    0.25\n",
      "ok\n",
      "Trying:\n",
      "    JD(Counter({0, 1, 2, 3}), Counter({10, 11, 12, 13}))\n",
      "Expecting:\n",
      "    1.0\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(JD, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c95be3",
   "metadata": {},
   "source": [
    "**Question 5**. As explained earlier, we need a function to precompute the distances between pairs of sequences and store them in an array. The main use of this is to calculate the pairwise distances between all pairs of n-grams derived from the training data (which is a list of sequences). The result of this is a square distance matrix, which will have zero on the diagonal. \n",
    "\n",
    "However, we can make our function slightly more general. If we write it to accept *two* lists of ngrams, and return a rectangular distance matrix then it will be useful in more situations. We can still use it for the original purpose by passing the same list twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55518c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(ngrams1, ngrams2):\n",
    "    \"\"\"\n",
    "    If ngrams1 and ngrams2 are the same we get a square matrix:\n",
    "    >>> distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                 count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).shape == (3, 3)\n",
    "    True\n",
    "\n",
    "    And we get zeros on the diagonal, because JD(A, A) == 0 for any A:\n",
    "    >>> distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                 count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).diagonal()\n",
    "    array([0., 0., 0.])\n",
    "\n",
    "    If ngrams1 and ngrams2 are different lengths we get a rectangular (not square) matrix:\n",
    "    >>> distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                 count_ngrams_multi(['abcd', 'abc'        ], 2)).shape == (3, 2)\n",
    "    True\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 5 lines\n",
    "    matrix = np.zeros((len(ngrams1), len(ngrams2)))\n",
    "    for j in range(len(ngrams2)):\n",
    "        for i in range(len(ngrams1)):\n",
    "            matrix[i][j] = JD(ngrams1[i], ngrams2[j])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9f031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                    count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).shape == (3, 3)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                    count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).diagonal()\n",
      "Expecting:\n",
      "    array([0., 0., 0.])\n",
      "ok\n",
      "Trying:\n",
      "    distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                    count_ngrams_multi(['abcd', 'abc'        ], 2)).shape == (3, 2)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(distance_matrix, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2094a21",
   "metadata": {},
   "source": [
    "**Question 6**. We need another helper function which calculates the *cluster score*. \n",
    "It will be similar to the $k$-means objective mentioned in lectures but not identical. We define the \n",
    "cluster score as 1 minus the normalised sum of intra-cluster distances. That is, we \n",
    "take all the points in a particular cluster and sum all the distances between them.\n",
    "Sum that over all clusters. Normalise that by dividing by the sum of distances \n",
    "between *all* points (regardless of cluster). The result is in [0, 1], of course. It's 0 if\n",
    "the points within each cluster are identical. It's larger if not. Finally, take 1\n",
    "minus that, so that higher is better. So 1 is the perfect score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c1d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_score(D, labels):\n",
    "    \"\"\"\n",
    "    Calculate a clustering score, defined as the 1 minus the\n",
    "    normalised sum of intra-cluster distances. The result is in\n",
    "    [0, 1], where higher is better.\n",
    "    \n",
    "    `D` is a square 2D array of distances between *all* points,\n",
    "    of shape (n, n) where n is the number of points.\n",
    "    \n",
    "    `labels` is a list or 1D array of cluster labels, of length n.\n",
    "    \n",
    "    Here we see two clusters of two points each and in each cluster, and\n",
    "    the points are at zero distance from each other,\n",
    "    so we get a perfect score of 1.0\n",
    "    >>> cluster_score(np.array([[0, 0, 1, 1], \n",
    "    ...                         [0, 0, 1, 1],\n",
    "    ...                         [1, 1, 0, 0], \n",
    "    ...                         [1, 1, 0, 0]]), \n",
    "    ...               [0, 0, 1, 1])\n",
    "    1.0\n",
    "    \n",
    "    Again, two clusters of two points each, but\n",
    "    this time they are clustered badly! Nearby points are in\n",
    "    different clusters.\n",
    "    >>> cluster_score(np.array([[0, 0, 1, 1], \n",
    "    ...                         [0, 0, 1, 1],\n",
    "    ...                         [1, 1, 0, 0], \n",
    "    ...                         [1, 1, 0, 0]]), \n",
    "    ...               [0, 1, 0, 1])\n",
    "    0.5\n",
    "    \n",
    "    Again, two clusters, but more realistic:\n",
    "    >>> round(cluster_score(np.array([[0, 1, 7, 8], \n",
    "    ...                               [1, 0, 6, 5],\n",
    "    ...                               [7, 6, 0, 1], \n",
    "    ...                               [8, 5, 1, 0]]), \n",
    "    ...                     [0, 0, 1, 1]), 3)\n",
    "    0.929\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 5 lines\n",
    "    sum = 0; den = 0; list1 = labels\n",
    "    for j in range(len(list1)):\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == list1[j]:\n",
    "                sum = (sum + D[i][j]) \n",
    "            if D[i][j] != 0:\n",
    "                den = den + D[i][j]\n",
    "    return 1 - (sum/den)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b006ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    cluster_score(np.array([[0, 0, 1, 1], \n",
      "                            [0, 0, 1, 1],\n",
      "                            [1, 1, 0, 0], \n",
      "                            [1, 1, 0, 0]]), \n",
      "                  [0, 0, 1, 1])\n",
      "Expecting:\n",
      "    1.0\n",
      "ok\n",
      "Trying:\n",
      "    cluster_score(np.array([[0, 0, 1, 1], \n",
      "                            [0, 0, 1, 1],\n",
      "                            [1, 1, 0, 0], \n",
      "                            [1, 1, 0, 0]]), \n",
      "                  [0, 1, 0, 1])\n",
      "Expecting:\n",
      "    0.5\n",
      "ok\n",
      "Trying:\n",
      "    round(cluster_score(np.array([[0, 1, 7, 8], \n",
      "                                  [1, 0, 6, 5],\n",
      "                                  [7, 6, 0, 1], \n",
      "                                  [8, 5, 1, 0]]), \n",
      "                        [0, 0, 1, 1]), 3)\n",
      "Expecting:\n",
      "    0.929\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(cluster_score, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad521eb",
   "metadata": {},
   "source": [
    "**Question 7**. Now, we're ready to put everything together. Create a class `NGramsKClusters` which inherits from `BaseEstimator` and `ClusterMixin`. This is analogous to the nearest-neighbours class we created when studying the Scikit-Learn API.\n",
    "\n",
    "It should have an `__init__` function which stores the values of $n$ and $k$. It should have a `fit` method, where the input `X` is a list of sequences. In `fit` we should precompute the distance matrix.\n",
    "\n",
    "To actually do the clustering in `fit`, we should use the Scikit-Learn `AgglomerativeClustering` method. We should pass it the arguments `linkage=\"average\"` and `affinity=\"precomputed\"`, but we don't need to understand the details here.\n",
    "\n",
    "Inside `fit`, our class should store the $n$-grams `ngrams_`, the distance matrix `distances_`, the labels found by AgglomerativeClustering `labels_`, and the clustering score `score_`. \n",
    "\n",
    "The function `fit_predict` will be supplied by the API. We don't have to write it. Behind the scenes it will call `fit` and immediately return the cluster label for each point in `X`. This is useful for clustering, because typically our only goal is to get cluster labels for the training data, ie we don't later on want to predict with any other `X`.\n",
    "\n",
    "However in our case, we will also write `predict`. The input to this is a new `X` and for each sequence in that `X` we should find the label of the nearest sequence in our training data. This can use our existing `ngrams_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3846ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramsKClusters(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"\n",
    "    >>> NGramsKClusters(n_clusters=2, ngram_len=3)\n",
    "    NGramsKClusters(n_clusters=2, ngram_len=3)\n",
    "\n",
    "    Don't forget, fit() should return self:\n",
    "    >>> NGramsKClusters(n_clusters=2, ngram_len=3).fit(X)\n",
    "    NGramsKClusters(n_clusters=2, ngram_len=3)\n",
    "    \n",
    "    If we fit with 2 clusters the output should be 0's and 1's\n",
    "    >>> set(NGramsKClusters(n_clusters=2, ngram_len=3).fit_predict(X))\n",
    "    {0, 1}\n",
    "\n",
    "    If we fit with 2 clusters the output should be 0's and 1's and 2's\n",
    "    >>> set(NGramsKClusters(n_clusters=3, ngram_len=3).fit_predict(X))\n",
    "    {0, 1, 2}\n",
    "\n",
    "    After fitting, the object should have various trailing-underscore values:\n",
    "    >>> nk = NGramsKClusters(n_clusters=2, ngram_len=3).fit(X)\n",
    "    >>> all(hasattr(nk, name)\n",
    "    ...     for name in ['ngrams_', 'labels_', 'score_', 'distances_'])\n",
    "    True\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # my code is ~20 lines\n",
    "    def __init__(self, n_clusters, ngram_len):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.ngram_len = ngram_len\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.ngrams_ = count_ngrams_multi(X, self.ngram_len)\n",
    "        self.distances_ = distance_matrix(self.ngrams_, self.ngrams_)\n",
    "        self.algoo= AgglomerativeClustering(n_clusters = self.n_clusters, linkage = 'average', affinity = 'precomputed')\n",
    "        self.labels_ = self.algoo.fit_predict(self.distances_)\n",
    "        self.score_ = cluster_score(self.distances_, self.labels_)\n",
    "        return self\n",
    "    \n",
    "    # def fit_predict(self, X, y=None):\n",
    "    #     return super().fit_predict(self.distances_)\n",
    "\n",
    "    # def predict(self, X, y=None):\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59997156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49347907024598825"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramsKClusters(n_clusters=2, ngram_len=3).fit(X)\n",
    "model.score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "997bf56b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "source code not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdoctest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_docstring_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNGramsKClusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptionflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNORMALIZE_WHITESPACE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\doctest.py:2125\u001b[0m, in \u001b[0;36mrun_docstring_examples\u001b[1;34m(f, globs, verbose, name, compileflags, optionflags)\u001b[0m\n\u001b[0;32m   2123\u001b[0m finder \u001b[38;5;241m=\u001b[39m DocTestFinder(verbose\u001b[38;5;241m=\u001b[39mverbose, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2124\u001b[0m runner \u001b[38;5;241m=\u001b[39m DocTestRunner(verbose\u001b[38;5;241m=\u001b[39mverbose, optionflags\u001b[38;5;241m=\u001b[39moptionflags)\n\u001b[1;32m-> 2125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2126\u001b[0m     runner\u001b[38;5;241m.\u001b[39mrun(test, compileflags\u001b[38;5;241m=\u001b[39mcompileflags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\doctest.py:901\u001b[0m, in \u001b[0;36mDocTestFinder.find\u001b[1;34m(self, obj, name, module, globs, extraglobs)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;66;03m# Read the module's source code.  This is used by\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# DocTestFinder._find_lineno to find the line number for a\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;66;03m# given object's docstring.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     source_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:817\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsourcefile\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the filename that can be used to locate an object's source.\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;124;03m    Return None if no way can be identified to get the source.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 817\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mgetfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m     all_bytecode_suffixes \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mDEBUG_BYTECODE_SUFFIXES[:]\n\u001b[0;32m    819\u001b[0m     all_bytecode_suffixes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mOPTIMIZED_BYTECODE_SUFFIXES[:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:785\u001b[0m, in \u001b[0;36mgetfile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 785\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is a built-in class\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mobject\u001b[39m))\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismethod(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[1;31mOSError\u001b[0m: source code not available"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(NGramsKClusters, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f0739",
   "metadata": {},
   "source": [
    "**Question 8**. Now, let's use our class and the data it stores to evaluate.\n",
    "\n",
    "First, write a line of code to create the `NGramsKClusters` object with $n=3$ and $k=3$. Then fit it with `X`. Then print out the score. This doesn't need to be in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is 3 lines\n",
    "model = NGramsKClusters(n_clusters=3, ngram_len=3)\n",
    "model.fit(X)\n",
    "print(model.score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73729055",
   "metadata": {},
   "source": [
    "**Question 9**. Next, here's a query sequence. Which cluster would it go into? Write a line or two of code to answer the question and to show that this result is \"correct\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [[0, 2, 1, 0, 1, 2, 0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a573a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is ~ 6 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdbce6",
   "metadata": {},
   "source": [
    "**Question 10**. Next let's visualise the results. First, use `imshow` to visualise the square distance matrix which has been saved by `fit`. We should get a result like this. It shows that the first 50 in X are all quite similar, and the last 50 in X are all quite similar, and the middle 50 are similar but slightly less so:\n",
    "\n",
    "![\"Title\"](NGramsKClusters_dissimilarity_matrix.png \"alt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is 1 line\n",
    "plt.imshow(model.distances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9970ccf",
   "metadata": {},
   "source": [
    "**Question 11**. Next, let's use $t$-SNE to visualise the data in 2D, with the labels shown as colour. This is similar to what we did under *Representation Learning*. Remember that we have `precomputed` distances. We should get an image something like this (it might be rotated or slightly different in other ways). It shows the three clusters clearly.\n",
    "\n",
    "![Image](NGramsKClusters_embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is 4 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096dc55",
   "metadata": {},
   "source": [
    "**Question 12**. Finally, let's check out how the score changes for different $k$. Again, this is similar to something we did under *Representation Learning*. We should end up with an image something like this:\n",
    "\n",
    "![Image](NGramsKClusters_cluster_score_by_k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5342ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. 5. 6. 7. 8.]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is ~ 5 lines\n",
    "k_var = np.linspace(2,8,7)\n",
    "for i in k_var:\n",
    "    model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0347a505dc9e7c92277165a1ea30f391a4c6a11f93c6bbbe5c249ca17fd9d013"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
